ğŸŒªï¸ NLP for Disaster Tweet Classification
Can AI save lives? This project uses Natural Language Processing (NLP) to classify tweets as real disaster alerts or irrelevant content?
Built on BERT for bidirectional context understanding, the model helps emergency services filter signal from noise on social media in real time.

ğŸš¨ Problem Statement
In the early moments of a disaster, Twitter is often the first source of informationâ€”but it's cluttered with sarcasm, humor, and false alarms. Misclassifying these tweets can lead to:

>Panic due to false positives

>Missed responses to real disasters

>Spread of misinformation by news or public agencies

>This project aims to solve that by classifying tweets as either:

>âœ… Real Disaster: "Massive earthquake hits Japan. 7.5 magnitude."

>âŒ Not a Disaster: "My phone diedâ€¦ total disaster ğŸ˜©"

ğŸ§ª Models Compared
Model           	Accuracy	   Comments-
>Regression      	66â€“68%	     Fails on context & sarcasm
>Neural Network	  73%	         Misses word sequence understanding
>BERT	            F1 = 0.82	   Best for contextual understanding, but resource-heavy

ğŸ¤– Why BERT?
>ğŸ“Œ Handles negation & sarcasm effectively

>ğŸ“Œ Uses self-attention to consider word order and context

>ğŸ“Œ Processes tweets bidirectionally, unlike traditional models

>âœ… Identifies phrases like â€œnot a disasterâ€ vs. â€œtotal disasterâ€ with high precision

âš™ï¸ How BERT Works (Simplified)
>Tokenization into subwords

>Special tokens like [CLS] and [SEP]

>Word & positional embeddings

>Self-attention over entire sequence

>Contextual sentiment/negation learning

>Final classification â†’ Disaster or Not

ğŸ§° Tech Stack
>Python, Pandas, Sklearn, PyTorch

>HuggingFace Transformers (BERT, DistilBERT)

>Jupyter Notebooks

>Dataset: 10,000 labeled tweets from Kaggle

ğŸ“‚ Project Structure
.
â”œâ”€â”€ data/                    # Labeled tweet dataset
â”œâ”€â”€ preprocessing/           # Cleaning & tokenization scripts
â”œâ”€â”€ models/                  # BERT and baseline models
â”œâ”€â”€ notebooks/               # EDA and experimentation
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

ğŸ§  Key Takeaways
>BERT achieves superior classification through transfer learning and contextual awareness.

>False disaster tweets are filtered, helping emergency services prioritize real-time responses.

>Future work: Real-time deployment using DistilBERT, mobile integration for field agents.

ğŸ‘¤ Author
Harshal Amin
ğŸ“ MSBAIM, Purdue University
